{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2*\n",
    "\n",
    "# Sprint Challenge - Neural Network Foundations\n",
    "\n",
    "Table of Problems\n",
    "\n",
    "1. [Defining Neural Networks](#Q1)\n",
    "2. [Chocolate Gummy Bears](#Q2)\n",
    "    - Perceptron\n",
    "    - Multilayer Perceptron\n",
    "4. [Keras MMP](#Q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Q1\"></a>\n",
    "## 1. Define the following terms:\n",
    "\n",
    "- **Neuron:** In a biology and in ANNs a neuron is a elemental unit of information transfer -- it basically acts like a function, taking in information from one or more inputs and spits out an output. \n",
    "- **Input Layer:** The input layer is where information comes into an ANN. I think the number of neurons in this layer is equal to the number of features / variables in the data that is to be modeled. \n",
    "- **Hidden Layer:** A hidden layer is one of the intermediate staging areas of processing data in a NN. It takes in the output of the layer before it and applies it's own wieghts (sometimes biases) and dropout thresholds. In essence, hidden layers allow for a NN to adapt to subtle diferences in a feature space, but honestly, I don't know how they do that. \n",
    "- **Output Layer:** Output layer is where our NN makes a prediction based upon it's input and all the changes applied throughout the NN.\n",
    "- **Activation:** Not totally clear on this, but after a layer has it's weights dot producted with the values of the previous layer, and \"activation\" function applied (eg sigmoid) which translates the values into something that can be passed to the next layer / the output. \n",
    "- **Backpropagation:** At each epoch, cycling all the layer of the NN, backprop readjusts the weights based upon a comparison of the output and the true answer. In other words, backpropigation is a method for decreasing error on each epoch by adjusting forward prop parameters (esp weight). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chocolate Gummy Bears <a id=\"Q2\"></a>\n",
    "\n",
    "Right now, you're probably thinking, \"yuck, who the hell would eat that?\". Great question. Your candy company wants to know too. And you thought I was kidding about the [Chocolate Gummy Bears](https://nuts.com/chocolatessweets/gummies/gummy-bears/milk-gummy-bears.html?utm_source=google&utm_medium=cpc&adpos=1o1&gclid=Cj0KCQjwrfvsBRD7ARIsAKuDvMOZrysDku3jGuWaDqf9TrV3x5JLXt1eqnVhN0KM6fMcbA1nod3h8AwaAvWwEALw_wcB). \n",
    "\n",
    "Let's assume that a candy company has gone out and collected information on the types of Halloween candy kids ate. Our candy company wants to predict the eating behavior of witches, warlocks, and ghosts -- aka costumed kids. They shared a sample dataset with us. Each row represents a piece of candy that a costumed child was presented with during \"trick\" or \"treat\". We know if the candy was `chocolate` (or not chocolate) or `gummy` (or not gummy). Your goal is to predict if the costumed kid `ate` the piece of candy. \n",
    "\n",
    "If both chocolate and gummy equal one, you've got a chocolate gummy bear on your hands!?!?!\n",
    "![Chocolate Gummy Bear](https://ed910ae2d60f0d25bcb8-80550f96b5feb12604f4f720bfefb46d.ssl.cf1.rackcdn.com/3fb630c04435b7b5-2leZuM7_-zoom.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "candy = pd.read_csv('chocolate_gummy_bears.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5000\n",
       "0    5000\n",
       "Name: ate, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy['ate'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chocolate</th>\n",
       "      <th>gummy</th>\n",
       "      <th>ate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chocolate  gummy  ate\n",
       "0          0      1    1\n",
       "1          1      0    1\n",
       "2          0      1    1\n",
       "3          0      0    0\n",
       "4          1      1    0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "To make predictions on the `candy` dataframe. Build and train a Perceptron using numpy. Your target column is `ate` and your features: `chocolate` and `gummy`. Do not do any feature engineering. :P\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why you could not achieve a higher accuracy with a *simple perceptron*. It's possible to achieve ~95% accuracy on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# Start your candy perceptron here\n",
    "\n",
    "X = candy[['chocolate', 'gummy']].values\n",
    "y = candy['ate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([y]).T\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "    def __init__(self, rate = 0.01, niter = 10):\n",
    "        self.rate = rate\n",
    "        self.niter = niter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data\n",
    "        X : Training vectors, X.shape : [#samples, #features]\n",
    "        y : Target values, y.shape : [#samples]\n",
    "        \"\"\"\n",
    "\n",
    "        # weights\n",
    "        self.weight = np.zeros(1 + X.shape[1])\n",
    "\n",
    "        # Number of misclassifications\n",
    "        self.errors = []  # Number of misclassifications\n",
    "\n",
    "        for i in range(self.niter):\n",
    "            err = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                delta_w = self.rate * (target - self.predict(xi))\n",
    "                self.weight[1:] += delta_w * xi\n",
    "                self.weight[0] += delta_w\n",
    "                err += int(delta_w != 0.0)\n",
    "                self.errors.append(err)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.weight[1:]) + self.weight[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn = Perceptron(0.01, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Perceptron at 0x7ffa2c661f60>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(pn.errors) + 1), pn.errors, marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Number of misclassifications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Come back to this.... Haven't looked at accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pn.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ugh. Not good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refelction: I didn't expect great results, but these just seem wrong. \n",
    "# Being able to adjust due to error would have been the main issue I would \n",
    "# expect with a simple, one layer-feedforward, dense NN. I also don't have \n",
    "# any control of the finer tune nobs to tweek the model and make it work. \n",
    "# I don't know what's going on here, but the issue I fought with most in the \n",
    "# assignment was getting the model to do anything but converge towards one or \n",
    "# another value for the whole prediction. I think it has to do with the weights changing \n",
    "# too rapidly between epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron <a id=\"Q3\"></a>\n",
    "\n",
    "Using the sample candy dataset, implement a Neural Network Multilayer Perceptron class that uses backpropagation to update the network's weights. Your Multilayer Perceptron should be implemented in Numpy. \n",
    "Your network must have one hidden layer.\n",
    "\n",
    "Once you've trained your model, report your accuracy. Explain why your MLP's performance is considerably better than your simple perceptron's on the candy dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# I want activations that correspond to negative weights to be lower\n",
    "# and activations that correspond to positive weights to be higher\n",
    "\n",
    "class NeuralNetwork3:\n",
    "    def __init__(self, inputs=2, h_nodes=3, output=1):\n",
    "        # Set up Architecture of Neural Network\n",
    "        self.inputs = inputs\n",
    "        self.hiddenNodes = h_nodes\n",
    "        self.outputNodes = output\n",
    "\n",
    "        # Initial Weights\n",
    "        # 2x3 Matrix Array for the First Layer\n",
    "        np.random.seed(421)\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes) * .01\n",
    "       \n",
    "        # 3x1 Matrix Array for Hidden to Output\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes) * .01\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        aka \"predict\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted sum of inputs => hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "        \n",
    "        # Activations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "        \n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "        \n",
    "        # Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "        \n",
    "        return self.activated_output\n",
    "        \n",
    "    def backward(self, X,y,o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        \n",
    "        # Error in Output\n",
    "        self.o_error = y - o\n",
    "        \n",
    "        # Apply Derivative of Sigmoid to error\n",
    "        # How far off are we in relation to the Sigmoid f(x) of the output\n",
    "        # ^- aka hidden => output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        # z2 error\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T)\n",
    "        # How much of that \"far off\" can explained by the input => hidden\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.activated_hidden)\n",
    "        \n",
    "        # Adjustment to first set of weights (input => hidden)\n",
    "        self.weights1 += X.T.dot(self.z2_delta)\n",
    "        # Adjustment to second set of weights (hidden => output)\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta)\n",
    "        \n",
    "\n",
    "    def train(self, X, y, n_itr):\n",
    "        for i in range(n_itr):\n",
    "            o = self.feed_forward(X)\n",
    "            self.backward(X,y,o)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        print('Predicted Output: \\n' + str(self.feed_forward(X)))\n",
    "        return\n",
    "\n",
    "    def verbose(self, X, y, n_itr):\n",
    "        for i in range(n_itr):\n",
    "            if (i+1 in [1,2,3,4,5]) or ((i+1) % 1000 ==0):\n",
    "                print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "                print('Input: \\n', X)\n",
    "                print('Actual Output: \\n', y)\n",
    "                print('Predicted Output: \\n', str(self.feed_forward(X)))\n",
    "                print(\"Loss: \\n\", str(np.mean(np.square(y - self.feed_forward(X)))))\n",
    "            o = self.feed_forward(X)\n",
    "            self.backward(X,y,o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = NeuralNetwork3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output: \n",
      "[[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n"
     ]
    }
   ],
   "source": [
    "NN.train(X, y, 1000)\n",
    "NN.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.50185695]\n",
      " [0.50185896]\n",
      " [0.50185695]\n",
      " ...\n",
      " [0.50185695]\n",
      " [0.50185695]\n",
      " [0.50185896]]\n",
      "Loss: \n",
      " 0.25000344595993035\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.02971935]\n",
      " [0.02967946]\n",
      " [0.02971935]\n",
      " ...\n",
      " [0.02971935]\n",
      " [0.02971935]\n",
      " [0.02967946]]\n",
      "Loss: \n",
      " 0.47118263120403464\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.4247499999999999\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.4247499999999999\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " ...\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "Actual Output: \n",
      " [[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Predicted Output: \n",
      " [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " ...\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]]\n",
      "Loss: \n",
      " 0.4247499999999999\n"
     ]
    }
   ],
   "source": [
    "NN.verbose(X, y, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NN.feed_forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 5000]\n",
      " [   1 5000]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.000e-01 2.502e+03]\n",
      " [5.000e-01 4.980e+03]\n",
      " [1.000e+00 2.518e+03]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5],\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       ...,\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       [0.5]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.round(y_pred, 0)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 2502]\n",
      " [   1 7498]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_pred, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7222\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well, it's a number. A slightly more reasonable number, but I need to tweek \n",
    "# and adjust the model manually to get better results. Already, this makes way \n",
    "# more sense than the last one. Maybe I'm not rounding at the end of the first \n",
    "# model. I realized when I started the SC that I hadn't built an accuracy function \n",
    "# in any of my perceptron classes. They also didn't exist in any of the lecture \n",
    "# perceptron classes. I guess I should have known I would need it and should have \n",
    "# built it in advance. I'm sure I could have made an iterative function to avoid an \n",
    "# import from sklearn, but it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S. Don't try candy gummy bears. They're disgusting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Keras MMP <a id=\"Q3\"></a>\n",
    "\n",
    "Implement a Multilayer Perceptron architecture of your choosing using the Keras library. Train your model and report its baseline accuracy. Then hyperparameter tune at least two parameters and report your model's accuracy.\n",
    "Use the Heart Disease Dataset (binary classification)\n",
    "Use an appropriate loss function for a binary classification task\n",
    "Use an appropriate activation function on the final layer of your network.\n",
    "Train your model using verbose output for ease of grading.\n",
    "Use GridSearchCV or RandomSearchCV to hyperparameter tune your model. (for at least two hyperparameters)\n",
    "When hyperparameter tuning, show you work by adding code cells for each new experiment.\n",
    "Report the accuracy for each combination of hyperparameters as you test them so that we can easily see which resulted in the highest accuracy.\n",
    "You must hyperparameter tune at least 3 parameters in order to get a 3 on this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(303, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>118</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>135</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "31    65    1   0       120   177    0        1      140      0      0.4   \n",
       "261   52    1   0       112   230    0        1      160      0      0.0   \n",
       "267   49    1   2       118   149    0        0      126      0      0.8   \n",
       "63    41    1   1       135   203    0        1      132      0      0.0   \n",
       "165   67    1   0       160   286    0        0      108      1      1.5   \n",
       "\n",
       "     slope  ca  thal  target  \n",
       "31       2   0     3       1  \n",
       "261      2   1     2       0  \n",
       "267      2   3     2       0  \n",
       "63       1   0     1       1  \n",
       "165      1   3     2       0  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ryanleeallred/datasets/master/heart.csv')\n",
    "df = df.sample(frac=1)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All numeric X, binary target y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y split\n",
    "\n",
    "y = df['target']\n",
    "X = df.drop(['target'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    165\n",
       "0    138\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "y_train = np.array([y_train]).T\n",
    "y_test = np.array([y_test]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# use Normalizer to transform array to 0,1\n",
    "x_train = Normalizer().fit(x_train).transform(x_train)\n",
    "x_test = Normalizer().fit(x_test).transform(x_test) # Hold off on running..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13323524, 0.00256222, 0.00256222, ..., 0.00512443, 0.        ,\n",
       "        0.00512443],\n",
       "       [0.13933   , 0.        , 0.00309622, ..., 0.00309622, 0.        ,\n",
       "        0.00619244],\n",
       "       [0.15059554, 0.        , 0.00278881, ..., 0.00557761, 0.00278881,\n",
       "        0.00557761],\n",
       "       ...,\n",
       "       [0.21573562, 0.0031266 , 0.00937981, ..., 0.0031266 , 0.0031266 ,\n",
       "        0.00625321],\n",
       "       [0.20562012, 0.00267039, 0.        , ..., 0.00534078, 0.00801117,\n",
       "        0.00534078],\n",
       "       [0.15474241, 0.        , 0.        , ..., 0.0061897 , 0.        ,\n",
       "        0.0061897 ]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 13)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 1)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 227 samples, validate on 76 samples\n",
      "Epoch 1/50\n",
      "227/227 [==============================] - 1s 3ms/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 2/50\n",
      "227/227 [==============================] - 0s 147us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 3/50\n",
      "227/227 [==============================] - 0s 142us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 4/50\n",
      "227/227 [==============================] - 0s 136us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 5/50\n",
      "227/227 [==============================] - 0s 150us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 6/50\n",
      "227/227 [==============================] - 0s 139us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 7/50\n",
      "227/227 [==============================] - 0s 140us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 8/50\n",
      "227/227 [==============================] - 0s 139us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 9/50\n",
      "227/227 [==============================] - 0s 138us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 10/50\n",
      "227/227 [==============================] - 0s 139us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 11/50\n",
      "227/227 [==============================] - 0s 133us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 12/50\n",
      "227/227 [==============================] - 0s 143us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 13/50\n",
      "227/227 [==============================] - 0s 139us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 14/50\n",
      "227/227 [==============================] - 0s 135us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 15/50\n",
      "227/227 [==============================] - 0s 133us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 16/50\n",
      "227/227 [==============================] - 0s 137us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 17/50\n",
      "227/227 [==============================] - 0s 144us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 18/50\n",
      "227/227 [==============================] - 0s 139us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 19/50\n",
      "227/227 [==============================] - 0s 142us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 20/50\n",
      "227/227 [==============================] - 0s 133us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 21/50\n",
      "227/227 [==============================] - 0s 137us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 22/50\n",
      "227/227 [==============================] - 0s 135us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 23/50\n",
      "227/227 [==============================] - 0s 135us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 24/50\n",
      "227/227 [==============================] - 0s 133us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 25/50\n",
      "227/227 [==============================] - 0s 136us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 26/50\n",
      "227/227 [==============================] - 0s 141us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 27/50\n",
      "227/227 [==============================] - 0s 131us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 28/50\n",
      "227/227 [==============================] - 0s 133us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 29/50\n",
      "227/227 [==============================] - 0s 135us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 30/50\n",
      "227/227 [==============================] - 0s 149us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 31/50\n",
      "227/227 [==============================] - 0s 142us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 32/50\n",
      "227/227 [==============================] - 0s 146us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 33/50\n",
      "227/227 [==============================] - 0s 140us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 34/50\n",
      "227/227 [==============================] - 0s 148us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 35/50\n",
      "227/227 [==============================] - 0s 148us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 36/50\n",
      "227/227 [==============================] - 0s 153us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 37/50\n",
      "227/227 [==============================] - 0s 146us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 38/50\n",
      "227/227 [==============================] - 0s 142us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 39/50\n",
      "227/227 [==============================] - 0s 144us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 40/50\n",
      "227/227 [==============================] - 0s 155us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 41/50\n",
      "227/227 [==============================] - 0s 144us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 42/50\n",
      "227/227 [==============================] - 0s 148us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 43/50\n",
      "227/227 [==============================] - 0s 143us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 44/50\n",
      "227/227 [==============================] - 0s 135us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 45/50\n",
      "227/227 [==============================] - 0s 137us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 46/50\n",
      "227/227 [==============================] - 0s 138us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 47/50\n",
      "227/227 [==============================] - 0s 135us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 48/50\n",
      "227/227 [==============================] - 0s 140us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 49/50\n",
      "227/227 [==============================] - 0s 135us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n",
      "Epoch 50/50\n",
      "227/227 [==============================] - 0s 127us/sample - loss: 8.4260 - accuracy: 0.4537 - val_loss: 8.3214 - val_accuracy: 0.4605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff992ad9198>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Important Hyperparameters\n",
    "inputs = x_train.shape[1]\n",
    "epochs = 50\n",
    "batch_size = 10\n",
    "\n",
    "\n",
    "# Create Model\n",
    "model = Sequential()\n",
    "model.add(Dense(17, activation='relu', input_dim=inputs))\n",
    "model.add(Dense(17, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "# Compile Model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit Model\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=epochs,\n",
    "          batch_size=epochs,\n",
    "#           verbose=True\n",
    "         )\n",
    "\n",
    "# Pretty horrible..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7709251216854818 using {'batch_size': 20, 'epochs': 200}\n",
      "Means: 0.7092510984332551, Stdev: 0.03399448911062261 with: {'batch_size': 20, 'epochs': 100}\n",
      "Means: 0.7709251216854818, Stdev: 0.03617461631741407 with: {'batch_size': 20, 'epochs': 200}\n",
      "Means: 0.6784140995420548, Stdev: 0.07216902143556027 with: {'batch_size': 40, 'epochs': 100}\n",
      "Means: 0.7356828133440227, Stdev: 0.023051216964497558 with: {'batch_size': 40, 'epochs': 200}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # There's a KerasRegressor too...\n",
    "\n",
    "# seed just for fun...\n",
    "seed = 777\n",
    "numpy.random.seed(seed)\n",
    "inputs = x_train.shape[1]\n",
    "\n",
    "X = x_train\n",
    "Y = y_train\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(21, input_dim=inputs, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=20, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [20, 40],\n",
    "              'epochs': [100, 200]\n",
    "              \n",
    "             }\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1) # make sure n_jobs is 1 for keras classifier...\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ah Ha!!!! more layers!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.7488986818275788 using {'learn_rate': 0.01, 'momentum': 0.3}\n",
      "Means: 0.7224669448604668, Stdev: 0.012335495235717326 with: {'learn_rate': 0.01, 'momentum': 0.1}\n",
      "Means: 0.7488986818275788, Stdev: 0.030149969590533194 with: {'learn_rate': 0.01, 'momentum': 0.3}\n",
      "Means: 0.7268722417071002, Stdev: 0.02856341512569334 with: {'learn_rate': 0.01, 'momentum': 0.5}\n",
      "Means: 0.7312775312016189, Stdev: 0.04187801271869096 with: {'learn_rate': 0.01, 'momentum': 0.8}\n",
      "Means: 0.7488986818275788, Stdev: 0.030149969590533194 with: {'learn_rate': 0.1, 'momentum': 0.1}\n",
      "Means: 0.7312775312016189, Stdev: 0.028746093972061283 with: {'learn_rate': 0.1, 'momentum': 0.3}\n",
      "Means: 0.7400881049391457, Stdev: 0.05588918627857447 with: {'learn_rate': 0.1, 'momentum': 0.5}\n",
      "Means: 0.7224669669168111, Stdev: 0.039196313165996644 with: {'learn_rate': 0.1, 'momentum': 0.8}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # There's a KerasRegressor too...\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 777\n",
    "numpy.random.seed(seed)\n",
    "inputs = x_train.shape[1]\n",
    "\n",
    "X = x_train\n",
    "Y = y_train\n",
    "\n",
    "optimizer = 'Adam'\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(learn_rate=0.01, momentum=0.1):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(21, input_dim=inputs, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=200, batch_size=20, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.01, 0.1]\n",
    "momentum = [0.1, 0.3, 0.5, 0.8]\n",
    "param_grid = dict(learn_rate=learn_rate, momentum=momentum)\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3) # make sure n_jobs is 1 in paperspace ... ?\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not impressed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.850220253551584 using {'batch_size': 10, 'epochs': 300}\n",
      "Means: 0.8149779893228136, Stdev: 0.05835236176604968 with: {'batch_size': 10, 'epochs': 200}\n",
      "Means: 0.850220253551584, Stdev: 0.023120880990635022 with: {'batch_size': 10, 'epochs': 300}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # There's a KerasRegressor too...\n",
    "\n",
    "# seed just for fun...\n",
    "seed = 777\n",
    "numpy.random.seed(seed)\n",
    "inputs = x_train.shape[1]\n",
    "\n",
    "X = x_train\n",
    "Y = y_train\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(21, input_dim=inputs, activation='linear'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=20, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "# batch_size = [10, 20, 40, 60, 80, 100]\n",
    "# param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# define the grid search parameters\n",
    "param_grid = {'batch_size': [10],\n",
    "              'epochs': [200, 300]\n",
    "              \n",
    "             }\n",
    "\n",
    "# Create Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1) # make sure n_jobs is 1 for keras classifier...\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# Report Results\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(f\"Means: {mean}, Stdev: {stdev} with: {param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above: changed initial activation to linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Hyperparameters\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"sprint_challenge\", entity=\"lambda-ds6\") #initializes and experiment\n",
    "\n",
    "\n",
    "X = x_train\n",
    "y = y_train\n",
    "\n",
    "inputs = X.shape[1]\n",
    "\n",
    "# wandb.config.epochs = 300    # notice the wandb --> tracks the changes to this variable\n",
    "# wandb.config.batch_size = 10\n",
    "\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(21, input_dim=inputs, activation='linear'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(14, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=300, batch_size=10, verbose=0)\n",
    "\n",
    "# Fit Model\n",
    "\n",
    "model.fit(X, y, \n",
    "          validation_split=0.2, \n",
    "          epochs=wandb.config.epochs,       # make sure your paremeters are right too...\n",
    "          batch_size=wandb.config.epochs, \n",
    "          callbacks=[WandbCallback()]       # where we send data to after evaluation\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
