#!/usr/bin/env python
# coding: utf-8

# <img align="left" src="https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png" width=200>
# <br></br>
# 
# # Neural Network Framework (Keras)
# 
# ## *Data Science Unit 4 Sprint 2 Assignmnet 3*
# 
# ## Use the Keras Library to build a Multi-Layer Perceptron Model on the Boston Housing dataset
# 
# - The Boston Housing dataset comes with the Keras library so use Keras to import it into your notebook. 
# - Normalize the data (all features should have roughly the same scale)
# - Import the type of model and layers that you will need from Keras.
# - Instantiate a model object and use `model.add()` to add layers to your model
# - Since this is a regression model you will have a single output node in the final layer.
# - Use activation functions that are appropriate for this task
# - Compile your model
# - Fit your model and report its accuracy in terms of Mean Squared Error
# - Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. 
# - Run this same data through a linear regression model. Which achieves higher accuracy?
# - Do a little bit of feature engineering and see how that affects your neural network model. (you will need to change your model to accept more inputs)
# - After feature engineering, which model sees a greater accuracy boost due to the new features?

# In[9]:


# Let's start simple?

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np
np.random.seed(421)


# In[10]:


from tensorflow.keras.datasets import boston_housing

(x_train, y_train), (x_test, y_test) = boston_housing.load_data()


# In[11]:


x_train.shape

# Pretty small


# In[12]:


x_test.shape


# In[78]:


# hmm...
y_train.shape


# In[ ]:


y_train


# In[13]:


import numpy as np

y_train = np.array([y_train]).T
y_test = np.array([y_test]).T

y_train.shape


# In[ ]:





# In[14]:


from sklearn.preprocessing import Normalizer

# use Normalizer to transform array to 0,1
x_train = Normalizer().fit(x_train).transform(x_train)
x_test = Normalizer().fit(x_test).transform(x_test) # Hold off on running...

y_train = Normalizer().fit(y_train).transform(y_train)
y_test = Normalizer().fit(y_test).transform(y_test) # Hold off on running...


# In[8]:





# In[14]:


#instantiate model : sequential means I can give one layer at a time? I think.

model = Sequential()


# In[16]:


# Baseline "vanilla" perceptron as model

# Changed the loss function for regression since our target is a bunch of floats

model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])


# In[ ]:





# In[ ]:





# In[ ]:


model.fit(x_train, y_train, epochs=150) #, batch_size=)


# In[ ]:





# In[ ]:





# In[ ]:


scores = model.evaluate(x_train,y_train)
print(f"{model.metrics_names[1]}: {scores[1]*100}")


# In[ ]:





# In[ ]:





# In[26]:


from tensorflow.keras.optimizers import SGD


# In[ ]:





# In[27]:




model_improved = Sequential(name="4layerPoo")

model_improved.add(Dense(17, input_dim=13, activation='relu', name='DenseFriend1'))
model_improved.add(Dense(5, activation='relu'))
model_improved.add(Dense(5, activation='relu'))
model_improved.add(Dense(1, activation='linear'))

opt = SGD(lr=0.01, momentum=0.9)
model_improved.compile(loss='mean_squared_error', optimizer=opt,
              metrics=['accuracy'])

# Let's inspect our new architecture
model_improved.summary()


# In[16]:


from matplotlib import pyplot as plt


# In[17]:


from tensorflow.keras.optimizers import SGD


# In[ ]:


from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np
from tensorflow.keras.optimizers import SGD
from matplotlib import pyplot as plt
from tensorflow.keras.datasets import boston_housing
from sklearn.preprocessing import Normalizer
np.random.seed(421)

(x_train, y_train), (x_test, y_test) = boston_housing.load_data()

y_train = np.array([y_train]).T
y_test = np.array([y_test]).T

# use Normalizer to transform array to 0,1
x_train = Normalizer().fit(x_train).transform(x_train)
x_test = Normalizer().fit(x_test).transform(x_test) # Hold off on running...

y_train = Normalizer().fit(y_train).transform(y_train)
y_test = Normalizer().fit(y_test).transform(y_test) # Hold off on running...


# In[18]:


# Fit the model, then evaluate (all in one block...)

model_improved = Sequential(name="4layerPoo")

model_improved.add(Dense(17, input_dim=13, activation='relu'))
model_improved.add(Dense(5, activation='relu'))
model_improved.add(Dense(5, activation='relu'))
model_improved.add(Dense(1, activation='linear'))

opt = SGD(lr=0.01, momentum=0.9)
model_improved.compile(loss='mean_squared_error', 
                       optimizer=opt,
                       metrics=['mse'])

history = model_improved.fit(x_train, y_train, 
                             validation_data=(x_test, y_test), 
                             epochs=100, 
                             verbose=False)

# evaluate the model
_, train_mse = model_improved.evaluate(x_train, y_train, verbose=0)
_, test_mse = model_improved.evaluate(x_test, y_test, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))

# plot loss during training
plt.subplot(211)
plt.title('MSE')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
# plot mse during training
plt.subplot(212)
plt.title('Mean Squared Error')
plt.plot(history.history['mse'], label='train')
plt.plot(history.history['val_mse'], label='test')
plt.legend()
plt.show()


# In[55]:





# In[56]:


import math

# evaluate the model
_, train_mse = model_improved.evaluate(x_train, y_train, verbose=0)
_, test_mse = model_improved.evaluate(x_test, y_test, verbose=0)
print('Train MSE: %.3f, Test MSE: %.3f' % (train_mse, test_mse))

# plot loss during training
# plt.subplot(211)
plt.title('MSE')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
plt.show()


# In[89]:


history.history['mse']


# In[87]:


score = model_improved.evaluate(x_train,y_train)
print(scores[0]*100)


# In[ ]:





# ## Use the Keras Library to build an image recognition network using the Fashion-MNIST dataset (also comes with keras)
# 
# - Load and preprocess the image data similar to how we preprocessed the MNIST data in class.
# - Make sure to one-hot encode your category labels
# - Make sure to have your final layer have as many nodes as the number of classes that you want to predict.
# - Try different hyperparameters. What is the highest accuracy that you are able to achieve.
# - Use the history object that is returned from model.fit to make graphs of the model's loss or train/validation accuracies by epoch. 
# - Remember that neural networks fall prey to randomness so you may need to run your model multiple times (or use Cross Validation) in order to tell if a change to a hyperparameter is truly producing better results.

# In[20]:


from tensorflow.keras.datasets import fashion_mnist

(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()


# In[43]:


(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()


# In[44]:


x_train.shape


# In[23]:


np.count_nonzero(x_train)


# In[29]:


28*28


# In[27]:


23423502/47040000


# In[ ]:


x_train[0]


# In[40]:


x_train = x_train.reshape(x_train.shape[0], 784)
x_test = x_test.reshape(x_test.shape[0], 784)

# # Normalize Our Data
# x_train = x_train / 255
# x_test = x_test / 255


# In[35]:


x_train.shape


# In[41]:


import numpy as np

y_train = np.array([y_train]).T
y_test = np.array([y_test]).T


# In[42]:


from sklearn.preprocessing import Normalizer

# use Normalizer to transform array to 0,1
x_train = Normalizer().fit(x_train).transform(x_train)
x_test = Normalizer().fit(x_test).transform(x_test) # Hold off on running...


# In[47]:


from tensorflow.keras import utils

y_train = utils.to_categorical(y_train, num_classes)
y_test = utils.to_categorical(y_test, num_classes)


# In[ ]:


from keras.layers import Dense
from keras.optimizers import SGD
from matplotlib import pyplot

model = Sequential()
model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))
model.add(Dense(3, activation='softmax'))


# In[ ]:





# In[ ]:





# In[ ]:


# Fit the model, then evaluate (all in one block...)

model_improved = Sequential(name="4layerPoo")

model_improved.add(Dense(17, input_dim=784, activation='relu'))
model_improved.add(Dense(5, activation='relu'))
model_improved.add(Dense(5, activation='relu'))
model_improved.add(Dense(1, activation='linear'))

opt = SGD(lr=0.01, momentum=0.9)
model_improved.compile(loss='mean_squared_error', 
                       optimizer=opt,
                       metrics=['mse'])

history = model_improved.fit(x_train, y_train, 
                             validation_data=(x_test, y_test), 
                             epochs=100, 
                             verbose=False)

# evaluate the model
_, train_mse = model_improved.evaluate(x_train, y_train, verbose=0)
_, test_mse = model_improved.evaluate(x_test, y_test, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))

# plot loss during training
plt.subplot(211)
plt.title('MSE')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='test')
plt.legend()
# plot mse during training
plt.subplot(212)
plt.title('Mean Squared Error')
plt.plot(history.history['mse'], label='train')
plt.plot(history.history['val_mse'], label='test')
plt.legend()
plt.show()


# In[ ]:





# In[ ]:





# In[ ]:





# In[ ]:





# ## Stretch Goals:
# 
# - Use Hyperparameter Tuning to make the accuracy of your models as high as possible. (error as low as possible)
# - Use Cross Validation techniques to get more consistent results with your model.
# - Use GridSearchCV to try different combinations of hyperparameters. 
# - Start looking into other types of Keras layers for CNNs and RNNs maybe try and build a CNN model for fashion-MNIST to see how the results compare.
